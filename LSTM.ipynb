{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "# Part 1 - Data Preprocessing\n",
    "\n",
    "# importing the libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# set the current working directory\n",
    "os.chdir(\"C:/Users/Pranav/Desktop/Recurrent_Neural_Networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import training set\n",
    "\n",
    "dataset_train = pd.read_csv(\"Google_Stock_Price_Train.csv\")\n",
    "training_set = dataset_train.iloc[:,1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 325.25],\n",
       "       [ 331.27],\n",
       "       [ 329.83],\n",
       "       ..., \n",
       "       [ 793.7 ],\n",
       "       [ 783.33],\n",
       "       [ 782.75]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = MinMaxScaler(feature_range= (0,1))\n",
    "\n",
    "training_set_scaled = sc.fit_transform(training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08581368],\n",
       "       [ 0.09701243],\n",
       "       [ 0.09433366],\n",
       "       ..., \n",
       "       [ 0.95725128],\n",
       "       [ 0.93796041],\n",
       "       [ 0.93688146]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a data structure with 60 timestamps and 1 output\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60,1258):\n",
    "    X_train.append(training_set_scaled[i-60:i , 0])\n",
    "    y_train.append(training_set_scaled[i , 0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08581368,  0.09701243,  0.09433366, ...,  0.07846566,\n",
       "         0.08034452,  0.08497656],\n",
       "       [ 0.09701243,  0.09433366,  0.09156187, ...,  0.08034452,\n",
       "         0.08497656,  0.08627874],\n",
       "       [ 0.09433366,  0.09156187,  0.07984225, ...,  0.08497656,\n",
       "         0.08627874,  0.08471612],\n",
       "       ..., \n",
       "       [ 0.92106928,  0.92438053,  0.93048218, ...,  0.95475854,\n",
       "         0.95204256,  0.95163331],\n",
       "       [ 0.92438053,  0.93048218,  0.9299055 , ...,  0.95204256,\n",
       "         0.95163331,  0.95725128],\n",
       "       [ 0.93048218,  0.9299055 ,  0.93113327, ...,  0.95163331,\n",
       "         0.95725128,  0.93796041]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08627874,  0.08471612,  0.07454052, ...,  0.95725128,\n",
       "        0.93796041,  0.93688146])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reshaping since RNN accepts only 3d shaped inputs\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Part 2 -- building the RNN\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising the RNN\n",
    "\n",
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding the first LSTM layer and some dropout regularization\n",
    "\n",
    "regressor.add(LSTM(units=50, return_sequences = True , input_shape = (X_train.shape[1], 1)))\n",
    "\n",
    "regressor.add(Dropout(0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding second layer of LSTM and dropout regularization\n",
    "\n",
    "regressor.add(LSTM(units=50, return_sequences = True ))\n",
    "\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding third layer of LSTM and dropout regularization\n",
    "\n",
    "regressor.add(LSTM(units=50, return_sequences = True ))\n",
    "\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding fourth layer of LSTM and dropout regularization. Remember return_sequences = False if we don't mention\n",
    "\n",
    "regressor.add(LSTM(units=50))\n",
    "\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "\n",
    "regressor.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# compiling the RNN\n",
    "\n",
    "regressor.compile(optimizer= 'adam' , loss= 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0540     \n",
      "Epoch 2/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0059     \n",
      "Epoch 3/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0059     \n",
      "Epoch 4/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0053     \n",
      "Epoch 5/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0056     \n",
      "Epoch 6/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0041     \n",
      "Epoch 7/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0040     \n",
      "Epoch 8/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0041     \n",
      "Epoch 9/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0044     \n",
      "Epoch 10/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0040     \n",
      "Epoch 11/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0041     \n",
      "Epoch 12/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0040     \n",
      "Epoch 13/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0038     \n",
      "Epoch 14/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0035     \n",
      "Epoch 15/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0036     \n",
      "Epoch 16/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0036     \n",
      "Epoch 17/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0034     \n",
      "Epoch 18/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0037     \n",
      "Epoch 19/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0035     \n",
      "Epoch 20/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0034     \n",
      "Epoch 21/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0035     \n",
      "Epoch 22/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0032     \n",
      "Epoch 23/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0033     \n",
      "Epoch 24/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0029     \n",
      "Epoch 25/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0031     \n",
      "Epoch 26/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0031     \n",
      "Epoch 27/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0036     \n",
      "Epoch 28/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0038     \n",
      "Epoch 29/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0029     \n",
      "Epoch 30/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0033     \n",
      "Epoch 31/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0033     \n",
      "Epoch 32/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0029     \n",
      "Epoch 33/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0027     \n",
      "Epoch 34/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0031     \n",
      "Epoch 35/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0026     \n",
      "Epoch 36/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0035     \n",
      "Epoch 37/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0028     \n",
      "Epoch 38/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0028     \n",
      "Epoch 39/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0025     \n",
      "Epoch 40/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0027     \n",
      "Epoch 41/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0025     \n",
      "Epoch 42/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0025     \n",
      "Epoch 43/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0025     \n",
      "Epoch 44/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0030     \n",
      "Epoch 45/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0027     \n",
      "Epoch 46/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0025     \n",
      "Epoch 47/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0023     \n",
      "Epoch 48/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0027     \n",
      "Epoch 49/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0021     \n",
      "Epoch 50/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0022     \n",
      "Epoch 51/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0030     \n",
      "Epoch 52/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0025     \n",
      "Epoch 53/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0021     \n",
      "Epoch 54/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0024     \n",
      "Epoch 55/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0022     \n",
      "Epoch 56/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0020     \n",
      "Epoch 57/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0019     \n",
      "Epoch 58/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0020     \n",
      "Epoch 59/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0018     \n",
      "Epoch 60/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0021     \n",
      "Epoch 61/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0019     \n",
      "Epoch 62/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0019     \n",
      "Epoch 63/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0017     \n",
      "Epoch 64/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0020     \n",
      "Epoch 65/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0019     \n",
      "Epoch 66/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0020     \n",
      "Epoch 67/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0016     \n",
      "Epoch 68/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0020     \n",
      "Epoch 69/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0018     \n",
      "Epoch 70/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0019     \n",
      "Epoch 71/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0017     \n",
      "Epoch 72/100\n",
      "1198/1198 [==============================] - 7s - loss: 0.0018     \n",
      "Epoch 73/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0016     \n",
      "Epoch 74/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0017     \n",
      "Epoch 75/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0018     \n",
      "Epoch 76/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0020     \n",
      "Epoch 77/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0018     \n",
      "Epoch 78/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0018     \n",
      "Epoch 79/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0017     \n",
      "Epoch 80/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0018     \n",
      "Epoch 81/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0017     \n",
      "Epoch 82/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0018     \n",
      "Epoch 83/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0020     \n",
      "Epoch 84/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0017     \n",
      "Epoch 85/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0018     \n",
      "Epoch 86/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0014     \n",
      "Epoch 87/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0017     \n",
      "Epoch 88/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0016     \n",
      "Epoch 89/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0016     \n",
      "Epoch 90/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0014     \n",
      "Epoch 91/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0016     \n",
      "Epoch 92/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0015     \n",
      "Epoch 93/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0014     \n",
      "Epoch 94/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0016     \n",
      "Epoch 95/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0015     \n",
      "Epoch 96/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0014     \n",
      "Epoch 97/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0014     \n",
      "Epoch 98/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0014     \n",
      "Epoch 99/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0016     \n",
      "Epoch 100/100\n",
      "1198/1198 [==============================] - 8s - loss: 0.0014     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1412ea47b38>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the RNN to the training set\n",
    "\n",
    "regressor.fit(X_train, y_train, epochs= 100, batch_size= 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making the predictions and visualising the results\n",
    "\n",
    "# getting the real stock price of 2017\n",
    "\n",
    "dataset_test = pd.read_csv(\"Google_Stock_Price_Test.csv\")\n",
    "\n",
    "real_stock_price = dataset_test.iloc[:,1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Getting the predicted stock price of 2017\n",
    "\n",
    "\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']) , axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       325.25\n",
       "1       331.27\n",
       "2       329.83\n",
       "3       328.34\n",
       "4       322.04\n",
       "5       313.70\n",
       "6       310.59\n",
       "7       314.43\n",
       "8       311.96\n",
       "9       314.81\n",
       "10      312.14\n",
       "11      319.30\n",
       "12      294.16\n",
       "13      291.91\n",
       "14      292.07\n",
       "15      287.68\n",
       "16      284.92\n",
       "17      284.32\n",
       "18      287.95\n",
       "19      290.41\n",
       "20      291.38\n",
       "21      291.34\n",
       "22      294.23\n",
       "23      296.39\n",
       "24      302.44\n",
       "25      303.18\n",
       "26      304.87\n",
       "27      302.81\n",
       "28      304.11\n",
       "29      304.63\n",
       "         ...  \n",
       "1248    800.40\n",
       "1249    790.22\n",
       "1250    796.76\n",
       "1251    795.84\n",
       "1252    792.36\n",
       "1253    790.90\n",
       "1254    790.68\n",
       "1255    793.70\n",
       "1256    783.33\n",
       "1257    782.75\n",
       "0       778.81\n",
       "1       788.36\n",
       "2       786.08\n",
       "3       795.26\n",
       "4       806.40\n",
       "5       807.86\n",
       "6       805.00\n",
       "7       807.14\n",
       "8       807.48\n",
       "9       807.08\n",
       "10      805.81\n",
       "11      805.12\n",
       "12      806.91\n",
       "13      807.25\n",
       "14      822.30\n",
       "15      829.62\n",
       "16      837.81\n",
       "17      834.71\n",
       "18      814.66\n",
       "19      796.86\n",
       "Name: Open, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60 : ].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 779.  ,  779.66,  777.71,  786.66,  783.76,  781.22,  781.65,\n",
       "        779.8 ,  787.85,  798.24,  803.3 ,  795.  ,  804.9 ,  816.68,\n",
       "        806.34,  801.  ,  808.35,  795.47,  782.89,  778.2 ,  767.25,\n",
       "        750.66,  774.5 ,  783.4 ,  779.94,  791.17,  756.54,  755.6 ,\n",
       "        746.97,  755.2 ,  766.92,  771.37,  762.61,  772.63,  767.73,\n",
       "        764.26,  760.  ,  771.53,  770.07,  757.44,  744.59,  757.71,\n",
       "        764.73,  761.  ,  772.48,  780.  ,  785.04,  793.9 ,  797.4 ,\n",
       "        797.34,  800.4 ,  790.22,  796.76,  795.84,  792.36,  790.9 ,\n",
       "        790.68,  793.7 ,  783.33,  782.75,  778.81,  788.36,  786.08,\n",
       "        795.26,  806.4 ,  807.86,  805.  ,  807.14,  807.48,  807.08,\n",
       "        805.81,  805.12,  806.91,  807.25,  822.3 ,  829.62,  837.81,\n",
       "        834.71,  814.66,  796.86])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = inputs.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 779.  ],\n",
       "       [ 779.66],\n",
       "       [ 777.71],\n",
       "       [ 786.66],\n",
       "       [ 783.76],\n",
       "       [ 781.22],\n",
       "       [ 781.65],\n",
       "       [ 779.8 ],\n",
       "       [ 787.85],\n",
       "       [ 798.24],\n",
       "       [ 803.3 ],\n",
       "       [ 795.  ],\n",
       "       [ 804.9 ],\n",
       "       [ 816.68],\n",
       "       [ 806.34],\n",
       "       [ 801.  ],\n",
       "       [ 808.35],\n",
       "       [ 795.47],\n",
       "       [ 782.89],\n",
       "       [ 778.2 ],\n",
       "       [ 767.25],\n",
       "       [ 750.66],\n",
       "       [ 774.5 ],\n",
       "       [ 783.4 ],\n",
       "       [ 779.94],\n",
       "       [ 791.17],\n",
       "       [ 756.54],\n",
       "       [ 755.6 ],\n",
       "       [ 746.97],\n",
       "       [ 755.2 ],\n",
       "       [ 766.92],\n",
       "       [ 771.37],\n",
       "       [ 762.61],\n",
       "       [ 772.63],\n",
       "       [ 767.73],\n",
       "       [ 764.26],\n",
       "       [ 760.  ],\n",
       "       [ 771.53],\n",
       "       [ 770.07],\n",
       "       [ 757.44],\n",
       "       [ 744.59],\n",
       "       [ 757.71],\n",
       "       [ 764.73],\n",
       "       [ 761.  ],\n",
       "       [ 772.48],\n",
       "       [ 780.  ],\n",
       "       [ 785.04],\n",
       "       [ 793.9 ],\n",
       "       [ 797.4 ],\n",
       "       [ 797.34],\n",
       "       [ 800.4 ],\n",
       "       [ 790.22],\n",
       "       [ 796.76],\n",
       "       [ 795.84],\n",
       "       [ 792.36],\n",
       "       [ 790.9 ],\n",
       "       [ 790.68],\n",
       "       [ 793.7 ],\n",
       "       [ 783.33],\n",
       "       [ 782.75],\n",
       "       [ 778.81],\n",
       "       [ 788.36],\n",
       "       [ 786.08],\n",
       "       [ 795.26],\n",
       "       [ 806.4 ],\n",
       "       [ 807.86],\n",
       "       [ 805.  ],\n",
       "       [ 807.14],\n",
       "       [ 807.48],\n",
       "       [ 807.08],\n",
       "       [ 805.81],\n",
       "       [ 805.12],\n",
       "       [ 806.91],\n",
       "       [ 807.25],\n",
       "       [ 822.3 ],\n",
       "       [ 829.62],\n",
       "       [ 837.81],\n",
       "       [ 834.71],\n",
       "       [ 814.66],\n",
       "       [ 796.86]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = sc.transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9299055 ],\n",
       "       [ 0.93113327],\n",
       "       [ 0.92750577],\n",
       "       [ 0.94415507],\n",
       "       [ 0.93876032],\n",
       "       [ 0.93403527],\n",
       "       [ 0.93483518],\n",
       "       [ 0.9313937 ],\n",
       "       [ 0.94636878],\n",
       "       [ 0.96569685],\n",
       "       [ 0.97510976],\n",
       "       [ 0.95966962],\n",
       "       [ 0.97808617],\n",
       "       [ 1.        ],\n",
       "       [ 0.98076494],\n",
       "       [ 0.97083116],\n",
       "       [ 0.98450406],\n",
       "       [ 0.96054394],\n",
       "       [ 0.9371419 ],\n",
       "       [ 0.92841729],\n",
       "       [ 0.90804747],\n",
       "       [ 0.8771858 ],\n",
       "       [ 0.92153434],\n",
       "       [ 0.93809063],\n",
       "       [ 0.93165414],\n",
       "       [ 0.95254483],\n",
       "       [ 0.88812412],\n",
       "       [ 0.88637547],\n",
       "       [ 0.87032145],\n",
       "       [ 0.88563137],\n",
       "       [ 0.90743359],\n",
       "       [ 0.91571173],\n",
       "       [ 0.89941588],\n",
       "       [ 0.91805566],\n",
       "       [ 0.9089404 ],\n",
       "       [ 0.9024853 ],\n",
       "       [ 0.89456061],\n",
       "       [ 0.91600938],\n",
       "       [ 0.9132934 ],\n",
       "       [ 0.88979835],\n",
       "       [ 0.86589404],\n",
       "       [ 0.89030062],\n",
       "       [ 0.90335962],\n",
       "       [ 0.89642086],\n",
       "       [ 0.91777662],\n",
       "       [ 0.93176576],\n",
       "       [ 0.94114145],\n",
       "       [ 0.95762334],\n",
       "       [ 0.96413424],\n",
       "       [ 0.96402262],\n",
       "       [ 0.96971501],\n",
       "       [ 0.95077759],\n",
       "       [ 0.96294367],\n",
       "       [ 0.96123223],\n",
       "       [ 0.95475854],\n",
       "       [ 0.95204256],\n",
       "       [ 0.95163331],\n",
       "       [ 0.95725128],\n",
       "       [ 0.93796041],\n",
       "       [ 0.93688146],\n",
       "       [ 0.92955205],\n",
       "       [ 0.94731751],\n",
       "       [ 0.94307612],\n",
       "       [ 0.96015329],\n",
       "       [ 0.98087655],\n",
       "       [ 0.98359253],\n",
       "       [ 0.97827219],\n",
       "       [ 0.98225314],\n",
       "       [ 0.98288563],\n",
       "       [ 0.98214153],\n",
       "       [ 0.979779  ],\n",
       "       [ 0.97849542],\n",
       "       [ 0.98182528],\n",
       "       [ 0.98245777],\n",
       "       [ 1.01045465],\n",
       "       [ 1.02407173],\n",
       "       [ 1.03930724],\n",
       "       [ 1.03354044],\n",
       "       [ 0.99624228],\n",
       "       [ 0.9631297 ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_test = []\n",
    "\n",
    "\n",
    "for i in range(60,80):\n",
    "    X_test.append(inputs[i-60:i , 0])\n",
    "    \n",
    "X_test= np.array(X_test)\n",
    "\n",
    "\n",
    "# Reshaping\n",
    "\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.9299055 ],\n",
       "        [ 0.93113327],\n",
       "        [ 0.92750577],\n",
       "        ..., \n",
       "        [ 0.95725128],\n",
       "        [ 0.93796041],\n",
       "        [ 0.93688146]],\n",
       "\n",
       "       [[ 0.93113327],\n",
       "        [ 0.92750577],\n",
       "        [ 0.94415507],\n",
       "        ..., \n",
       "        [ 0.93796041],\n",
       "        [ 0.93688146],\n",
       "        [ 0.92955205]],\n",
       "\n",
       "       [[ 0.92750577],\n",
       "        [ 0.94415507],\n",
       "        [ 0.93876032],\n",
       "        ..., \n",
       "        [ 0.93688146],\n",
       "        [ 0.92955205],\n",
       "        [ 0.94731751]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0.96054394],\n",
       "        [ 0.9371419 ],\n",
       "        [ 0.92841729],\n",
       "        ..., \n",
       "        [ 1.01045465],\n",
       "        [ 1.02407173],\n",
       "        [ 1.03930724]],\n",
       "\n",
       "       [[ 0.9371419 ],\n",
       "        [ 0.92841729],\n",
       "        [ 0.90804747],\n",
       "        ..., \n",
       "        [ 1.02407173],\n",
       "        [ 1.03930724],\n",
       "        [ 1.03354044]],\n",
       "\n",
       "       [[ 0.92841729],\n",
       "        [ 0.90804747],\n",
       "        [ 0.8771858 ],\n",
       "        ..., \n",
       "        [ 1.03930724],\n",
       "        [ 1.03354044],\n",
       "        [ 0.99624228]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_stock_price = regressor.predict(X_test)\n",
    "\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 771.19012451],\n",
       "       [ 768.3170166 ],\n",
       "       [ 769.29260254],\n",
       "       [ 771.33221436],\n",
       "       [ 775.08972168],\n",
       "       [ 781.26226807],\n",
       "       [ 786.23809814],\n",
       "       [ 787.26190186],\n",
       "       [ 786.71588135],\n",
       "       [ 786.40679932],\n",
       "       [ 786.6942749 ],\n",
       "       [ 787.03033447],\n",
       "       [ 787.20007324],\n",
       "       [ 787.90960693],\n",
       "       [ 788.84851074],\n",
       "       [ 793.99291992],\n",
       "       [ 801.36462402],\n",
       "       [ 808.57196045],\n",
       "       [ 811.23394775],\n",
       "       [ 804.63452148]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_stock_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Visualising the results\n",
    "\n",
    "plt.plot(real_stock_price , color = 'red' , label = \"Real Google Stock Price\")\n",
    "\n",
    "\n",
    "plt.plot(predicted_stock_price , color = 'blue' , label = \"Predicted Google Stock Price\")\n",
    "\n",
    "plt.title('Google Stock Price Prediction')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "\n",
    "plt.ylabel('Google Stock Price')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
